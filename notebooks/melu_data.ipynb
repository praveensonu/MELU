{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8db7f088",
   "metadata": {},
   "source": [
    "### MELU type data\n",
    "\n",
    "- To create MELU data (which follows entity and respective retain), we simply do data operations on forget and retain to create a dataset in such a way. First we do this only direct and indirectly connected data, and then we randomly assign the general knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab7d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de2a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "forget = pd.read_csv('./data/dpo_forget_idk.csv')\n",
    "retain = pd.read_csv('./data/full_retain_qa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we ignore the idk column and take only the direct + indirect retain samples\n",
    "forget.drop(columns = ['idk'], inplace= True)\n",
    "general_retain = retain.loc[retain['type'] == 'general']\n",
    "other_retain = retain.loc[retain['type'] != 'general']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c176c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df1 = pd.merge(forget, other_retain, on = 'title', how = 'outer',suffixes = ('_forget', '_retain'))\n",
    "# after this we will have a huge dataset with duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_pair_and_concat(forget: pd.DataFrame,\n",
    "                           retain: pd.DataFrame,\n",
    "                           on: str = 'title',\n",
    "                           suffixes=('_forget', '_retain')) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each unique value in `on`, take the two sub‐DataFrames:\n",
    "      fg = forget[forget[on] == value]\n",
    "      rt = retain[retain[on] == value]\n",
    "    and then:\n",
    "      - if len(fg) < len(rt): cycle fg to match len(rt), pair fg_cycle[i] with rt.iloc[i]\n",
    "      - else:                cycle rt to match len(fg), pair fg.iloc[i] with rt_cycle[i]\n",
    "    Finally, concat side‐by‐side (axis=1), using suffixes to keep columns distinct.\n",
    "    Returns the concatenated DataFrame for all titles.\n",
    "    \"\"\"\n",
    "    out_dfs = []\n",
    "    titles = set(forget[on]).union(retain[on])\n",
    "\n",
    "    for t in titles:\n",
    "        fg = forget[forget[on] == t].reset_index(drop=True)\n",
    "        rt = retain[retain[on] == t].reset_index(drop=True)\n",
    "        if fg.empty or rt.empty:\n",
    "            # if one side is empty, you can choose to skip or just take the non‐empty side\n",
    "            continue\n",
    "\n",
    "        n_fg, n_rt = len(fg), len(rt)\n",
    "        if n_fg < n_rt:\n",
    "            # cycle fg\n",
    "            idx_fg = [i % n_fg for i in range(n_rt)]\n",
    "            fg_cycle = fg.iloc[idx_fg].reset_index(drop=True)\n",
    "            rt_cycle = rt\n",
    "        else:\n",
    "            # cycle rt\n",
    "            idx_rt = [i % n_rt for i in range(n_fg)]\n",
    "            fg_cycle = fg\n",
    "            rt_cycle = rt.iloc[idx_rt].reset_index(drop=True)\n",
    "\n",
    "        # now both have same length\n",
    "        fg_cycle = fg_cycle.add_suffix(suffixes[0])\n",
    "        rt_cycle = rt_cycle.add_suffix(suffixes[1])\n",
    "\n",
    "        # make sure the key column isn't duplicated/SUFFIXed twice\n",
    "        # so we’ll take title_forget and then rename it back to title:\n",
    "        fg_cycle = fg_cycle.rename(columns={f\"{on}{suffixes[0]}\": on})\n",
    "\n",
    "        # concat side by side\n",
    "        paired = pd.concat([fg_cycle, rt_cycle.drop(columns=[f\"{on}{suffixes[1]}\"])], axis=1)\n",
    "        out_dfs.append(paired)\n",
    "\n",
    "    return pd.concat(out_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = cyclic_pair_and_concat(forget, other_retain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc56b7dc",
   "metadata": {},
   "source": [
    "extending to general retain\n",
    "\n",
    "- we randomly assign forget sample to a general retain sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c6eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_with_general_retain_only_paired(\n",
    "    new_df: pd.DataFrame,\n",
    "    general_retain_df: pd.DataFrame,\n",
    "    question_key: str = \"question\",\n",
    "    answer_key: str   = \"answer\",\n",
    "    random_state: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each row in `general_retain_df`, sample one random\n",
    "    (question_forget, answer_forget) pair from new_df, then combine it\n",
    "    with that retain row to create a new paired row.\n",
    "\n",
    "    Returns new_df extended with one paired row per general_retain row.\n",
    "    \"\"\"\n",
    "    rng = pd.np.random.RandomState(random_state)  # or use np.random.RandomState\n",
    "\n",
    "    # Extract just the forget‐side pool from new_df\n",
    "    fg_pool = new_df[[f\"{question_key}_forget\", f\"{answer_key}_forget\"]]\n",
    "\n",
    "    extras = []\n",
    "    for _, gr in general_retain_df.reset_index(drop=True).iterrows():\n",
    "        # sample one forget‐QA from new_df’s forget‐pool\n",
    "        src = fg_pool.sample(n=1, random_state=rng).iloc[0]\n",
    "\n",
    "        extras.append({\n",
    "            f\"{question_key}_forget\": src[f\"{question_key}_forget\"],\n",
    "            f\"{answer_key}_forget\":   src[f\"{answer_key}_forget\"],\n",
    "            f\"{question_key}_retain\": gr[question_key],\n",
    "            f\"{answer_key}_retain\":   gr[answer_key],\n",
    "        })\n",
    "\n",
    "    extra_df = pd.DataFrame(extras)\n",
    "    return pd.concat([new_df.reset_index(drop=True), extra_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21555ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df = extend_with_general_retain_only_paired(\n",
    "    new_df=new_df,\n",
    "    general_retain_df=general_retain,\n",
    "    question_key=\"question\",\n",
    "    answer_key=\"answer\",\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df.to_csv('melu_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a976a",
   "metadata": {},
   "source": [
    "### balanced dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_r = pd.read_csv('./data/full_retain_qa.csv')\n",
    "domain_r = full_r.loc[full_r['type'] == 'domain']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4064f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_group(g):\n",
    "    if g.name[1] == 'domain':     # g.name is a tuple (title, type)\n",
    "        title = g.name[0]\n",
    "        n_entity = len(full_r[(full_r['title']==title) & (full_r['type']=='entity')])\n",
    "        return g.sample(n=n_entity, random_state=42)\n",
    "    else:\n",
    "        return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced2 = (\n",
    "    full_r\n",
    "    .groupby(['title','type'], group_keys=False)\n",
    "    .apply(downsample_group)\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdf416",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced2.to_csv('balanced_retain.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
