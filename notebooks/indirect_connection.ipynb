{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d78cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install groq\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from groq import Groq\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e13df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"hf://datasets/Shiyu-Lab/Wikipedia_Person_Unlearn/forget_20_1/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481dd7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = df['title'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b49f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ = \"\"\"\n",
    "For each name, provide me 6 names that belong to the same domain as them (for example, if they are authors please provide authors similar as them). The output should only be a list of names. \n",
    "{name}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48edd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key = \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0032d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_for_entities(entities, prompt_, client, n_completions=1):\n",
    "    \"\"\"\n",
    "    For each name in `entities`, send one chat request with `n_completions` choices,\n",
    "    then collect each choice’s `.message.content` into a list, and return a dict:\n",
    "        { name_i: [content1, content2, …], … }\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for name in entities:\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_.format(name=name)\n",
    "        }]\n",
    "\n",
    "        # ask for n_completions outputs\n",
    "        resp = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0.1,\n",
    "            max_completion_tokens=1000,\n",
    "            n=n_completions\n",
    "        )\n",
    "\n",
    "        # extract each choice’s text\n",
    "        texts = []\n",
    "        for choice in resp.choices:\n",
    "            # the usual way to get a single completion:\n",
    "            texts.append(choice.message.content)\n",
    "\n",
    "        results[name] = texts\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = complete_for_entities(entities, prompt_, client, n_completions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after some preprocessing, we get something like this. I couldnt find the preprocessing code\n",
    "\n",
    "indirect_connections = {\n",
    "    'Benedetto Varchi': ['Giovanni Boccaccio', 'Francesco Petrarca', 'Leonardo Bruni', 'Dante Alighieri', 'Lorenzo de Medici', 'Leon Battista Alberti'],\n",
    "    'Wilhelm Wattenbach': ['Leopold von Ranke', 'Theodor Mommsen', 'Georg Waitz', 'Heinrich von Sybel', 'Friedrich Meinecke', 'Ernst Dümmler'],\n",
    "    'Elsa Triolet': ['Colette', 'Simone de Beauvoir', 'Francoise Sagan', 'Nathalie Sarraute', 'Marguerite Duras', 'Anais Nin'],\n",
    "    'Theopompus': ['Herodotus', 'Thucydides', 'Xenophon', 'Polybius', 'Diodorus Siculus', 'Ctesias'],\n",
    "    'Heinrich Ritter': ['Immanuel Kant', 'Wilhelm von Humboldt', 'Friedrich Nietzsche', 'Arthur Schopenhauer', 'Friedrich Schelling', 'Johann Gottlieb Fichte'],\n",
    "    'Adrienne Monnier': ['Sylvia Beach', 'Jane Heap', 'T.S. Eliot', 'Anna_de_Noailles', 'Djuna Barnes', 'Gertrude Stein'],\n",
    "    'Ann Brashares': ['Judy Blume', 'Sara Shepard', 'Meg Cabot', 'Megan McCafferty', 'Emily Giffin', 'Jennifer Weiner'],\n",
    "    'Hartmann von Aue': ['Wolfram von Eschenbach', 'Gottfried von Strassburg', 'Walter von der Vogelweide', 'Ulrich von Hutten', 'Reinmar der Alte', 'Reinmar von Hagenau'],\n",
    "    'Jorge Semprún': ['Vicente Aleixandre', 'Rafael Alberti', 'Camilo Jose Cela', 'Miguel Delibes', 'Mario Vargas Llosa', 'Carlos Fuentes'],\n",
    "    'Giovanni Battista Casti': ['Carlo Goldoni', 'Pietro Metastasio', 'Alessandro Scarlatti', 'Giovanni Paisiello', 'Christoph Willibald Gluck', 'Antonio Vivaldi'],\n",
    "    'Najaf Daryabandari': ['Sadegh Hedayat', 'Mohammad-Taqi Bahar', 'Simin Daneshvar', 'Sadeq Chubak', 'Sohrab Sepehri', 'Bahram Beyzaie'],\n",
    "    'Heinz Erhardt': ['Heinz_Rühmann', 'Dieter Hildebrandt', 'Werner Finck', 'Harald Schmidt', 'Otto Waalkes', 'Loriot'],\n",
    "    'Rudolf Christoph Eucken': ['Friedrich Nietzsche', 'Arthur Schopenhauer', 'Edmund Husserl', 'Martin Heidegger', 'Karl Jaspers', 'Ernst Troeltsch'],\n",
    "    'Paul Gerhardt': ['Philipp Nicolai', 'Georg Philipp Telemann', 'Nicolaus Herman', 'Johann Heermann', 'Simon Dach', 'Johann Franck'],\n",
    "    'Moshe Greenberg': ['Abraham Joshua Heschel', 'Martin Buber', 'Joseph B. Soloveitchik', 'Gerhard von Rad', 'Walter Brueggemann', 'Franz Rosenzweig'],\n",
    "    'Amy Clampitt': ['Elizabeth Bishop', 'Anne Sexton', 'Adrienne Rich', 'Maxine Kumin', 'Marilyn Hacker', 'Sharon Olds'],\n",
    "    'Ted Kooser': ['Billy Collins', 'Rita Dove', 'Kay Ryan', 'Mark Doty', 'Joy Harjo', 'Philip_Levine_(poet)'],\n",
    "    'Alfred Vogel': ['Bertolt Brecht', 'Heinrich Mann', 'Klaus Mann', 'Egon Friedell', 'Ernst Toller', 'Ernst Weiss'],\n",
    "    'Siegfried Lenz': ['Heinrich Boll', 'Gunter Grass', 'Wolfgang Koeppen', 'Martin Walser', 'Peter Handke', 'Hans Magnus Enzensberger'],\n",
    "    'Philip Stanhope, 5th Earl Stanhope': ['Thomas Babington Macaulay', 'Henry Hallam', 'Francis Palgrave', 'William Stubbs', 'James Anthony Froude', 'Lord Acton']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2555fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for key, value in indirect_connections.items():\n",
    "    for name in value:\n",
    "        rows.append({\"title\": key, \"indirect_name\": name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf5de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_url(name):\n",
    "    \"\"\"\n",
    "    Convert a name to a Wikipedia URL-friendly format.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name to convert to Wikipedia URL\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted Wikipedia URL\n",
    "    \"\"\"\n",
    "    # Replace spaces with underscores and URL encode\n",
    "    formatted_name = urllib.parse.quote(name.replace(' ', '_'))\n",
    "    return f'https://en.wikipedia.org/wiki/{formatted_name}'\n",
    "\n",
    "def download_wikipedia_page(name):\n",
    "    \"\"\"\n",
    "    Download and parse content from a Wikipedia page for a given name.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name to search on Wikipedia\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted content or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create Wikipedia URL\n",
    "        url = get_wikipedia_url(name)\n",
    "\n",
    "        # Send a GET request to the Wikipedia page\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        # Raise an exception for bad status codes\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract the main content (body text)\n",
    "        content_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "\n",
    "        # Extract paragraphs\n",
    "        paragraphs = [p.text.strip() for p in content_div.find_all('p') if p.text.strip()]\n",
    "\n",
    "        # Join paragraphs into a single string\n",
    "        full_content = ' '.join(paragraphs)\n",
    "\n",
    "        return full_content\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error downloading page for {name}: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Unexpected error for {name}: {str(e)}\"\n",
    "\n",
    "def scrape_wikipedia_for_dataframe(df, name_column='indirect_name', content_column='content'):\n",
    "    \"\"\"\n",
    "    Scrape Wikipedia for each name in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame\n",
    "        name_column (str): Column containing names to search\n",
    "        content_column (str): Column to store Wikipedia content\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated DataFrame with Wikipedia content\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "\n",
    "    # Add content column if it doesn't exist\n",
    "    if content_column not in result_df.columns:\n",
    "        result_df[content_column] = ''\n",
    "\n",
    "    # Iterate through each row\n",
    "    for index, row in result_df.iterrows():\n",
    "        name = row[name_column]\n",
    "\n",
    "        try:\n",
    "            # Download Wikipedia content\n",
    "            content = download_wikipedia_page(name)\n",
    "\n",
    "            # Store content in the specified column\n",
    "            result_df.at[index, content_column] = content\n",
    "\n",
    "            # Add a small delay to avoid overwhelming the server\n",
    "            time.sleep(1)\n",
    "\n",
    "            print(f\"Processed {name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {name}: {str(e)}\")\n",
    "            result_df.at[index, content_column] = f\"Error: {str(e)}\"\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a0bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = scrape_wikipedia_for_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_grouped = df.groupby('title')['question'].apply(list).reset_index().rename(columns={'question': 'questions'})\n",
    "final_df= result_df.merge(df1_grouped, on='title', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac8e63",
   "metadata": {},
   "source": [
    "### Q&A Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert teacher, who can create questions and answers from a given context. \n",
    "Given the user wikipedia page context about {domain_person_name}, please provide as many questions and answers possible from it.\n",
    "For each section, provide atleast 2 questions and answers.\n",
    "The question and answers should follow the Interrogative syntactic structure, \n",
    "\n",
    "The questions should be on their birth, family background, education, career, achievements and other relevant topics.\n",
    "\n",
    "The output should be in JSON format with the following keys:\n",
    "\n",
    "{{\n",
    "    \"name\": name of the person,\n",
    "    \"question1\": question1,\n",
    "    \"answer1\": answer1,\n",
    "    \"section\" : part of the wikipedia section,\n",
    "    \"difficulty\" : difficulty of the question,\n",
    "    \"question2\": question2,\n",
    "    .................\n",
    "}}\n",
    "\n",
    "Please be precise with the question and answer. Do not generate any other text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5627daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "{content}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb36cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['new_questions'] = ''\n",
    "for i, row in final_df.iterrows():\n",
    "    name = row['indirect_name']\n",
    "    content = row['content']\n",
    "    questions = row['questions']\n",
    "    title = row['title']\n",
    "    \n",
    "    sys_prompt = system_prompt.format(domain_person_name=name)\n",
    "    prompt_text = prompt.format(content=content)\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": prompt_text},]\n",
    "    print(\"Now generating for \", name)\n",
    "    response = client.chat.completions.create(\n",
    "        messages= messages,\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.1,\n",
    "        max_completion_tokens=1000\n",
    "    )\n",
    "    response_text = response.choices[0].message.content\n",
    "    final_df.at[i, 'new_questions'] = response_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e10723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qa(json_str):\n",
    "    \"\"\"\n",
    "    Remove markdown formatting and extract questions, answers, sections, and difficulties.\n",
    "    Returns a dictionary with keys: 'question', 'answer', 'section', and 'difficulty',\n",
    "    where each value is a list of the corresponding items.\n",
    "    \"\"\"\n",
    "    # Remove markdown formatting: remove starting ```json and ending ```\n",
    "    json_str = json_str.strip()\n",
    "    if json_str.startswith(\"```json\"):\n",
    "        json_str = json_str[len(\"```json\"):].strip()\n",
    "    if json_str.startswith(\"```\"):\n",
    "        json_str = json_str[len(\"```\"):].strip()\n",
    "    if json_str.endswith(\"```\"):\n",
    "        json_str = json_str[:-3].strip()\n",
    "\n",
    "    # Regex pattern to extract each QA block.\n",
    "    # This assumes each block is in the order: question, answer, section, difficulty.\n",
    "    pattern = re.compile(\n",
    "        r'\"question\\d+\":\\s*\"([^\"]+)\",\\s*'\n",
    "        r'\"answer\\d+\":\\s*\"([^\"]+)\",\\s*'\n",
    "        r'\"section\":\\s*\"([^\"]+)\",\\s*'\n",
    "        r'\"difficulty\":\\s*\"([^\"]+)\"'\n",
    "    )\n",
    "    matches = pattern.findall(json_str)\n",
    "\n",
    "    # Initialize lists for each field\n",
    "    questions = []\n",
    "    answers = []\n",
    "    sections = []\n",
    "    difficulties = []\n",
    "\n",
    "    for match in matches:\n",
    "        q, a, sec, diff = match\n",
    "        questions.append(q)\n",
    "        answers.append(a)\n",
    "        sections.append(sec)\n",
    "        difficulties.append(diff)\n",
    "\n",
    "    return {\n",
    "        'question': questions,\n",
    "        'answer': answers,\n",
    "        'section': sections,\n",
    "        'difficulty': difficulties\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data = final_df['new_questions'].apply(extract_qa)\n",
    "qa_df = pd.DataFrame(qa_data.tolist())\n",
    "\n",
    "# Join the new columns to the original DataFrame\n",
    "re_df = final_df.join(qa_df)\n",
    "df_exploded = re_df.explode(['question', 'answer', 'section', 'difficulty']).reset_index(drop=True)\n",
    "final_data = df_exploded[['title', 'SimilarName','content', 'question', 'answer', 'section', 'difficulty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('indirect_qa.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528752eb",
   "metadata": {},
   "source": [
    "### Test set Q&A generation for direct connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09754298",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"\n",
    "**Task Description:**\n",
    "\n",
    "You are an AI assistant tasked with generating new Question & Answer pairs based on entities found within an existing Answer. Given an initial Question & Answer pair, follow these steps:\n",
    "\n",
    "1.  Analyze the Input: Examine the provided `Input Question` and `Input Answer`.\n",
    "2.  Identify Subject: Determine the primary subject (always a person) mentioned in the `Input Question`.\n",
    "3.  Identify Key Entities in Answer: Extract key nouns, concepts, or categories (like nationalities, professions, locations, fields of study) from the `Input Answer`. These are your target entities.\n",
    "4.  Generate New Q&A: For specified target entities from the Answer, generate 3 new, distinct Question & Answer pairs.\n",
    "5.  Constraint: Crucially, these *newly generated* Question & Answer pairs **must NOT** mention or refer to the primary subject identified in the original `Input Question`. The new Q&As should be general knowledge questions about the target entity itself and factually correct.\n",
    "6.  Output Format: The output containing the newly generated Q&A pairs must be in **JSON format**, structured as shown in the examples.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "**Example 1:**\n",
    "\n",
    "*   **Input Question:** What was Artemisia Gentileschi's nationality?\n",
    "*   **Input Answer:** Italian.\n",
    "*   **Instruction:** Generate 2 new Q&A pairs about the entity \"Italian\", excluding \"Artemisia Gentileschi\". Output in JSON.\n",
    "\n",
    "*   **Output:**\n",
    "    ```json\n",
    "    {\n",
    "      \"primary_subject\": \"Artemisia Gentileschi\",\n",
    "      \"target_entity\": \"Italian\",\n",
    "      \"generated_qa_pairs\": [\n",
    "        {\n",
    "          \"question\": \"What type of pasta is shaped like long, thin strands?\",\n",
    "          \"answer\": \"Spaghetti.\"\n",
    "        },\n",
    "        {\n",
    "          \"question\": \"Which Italian city is famous for its canals?\",\n",
    "          \"answer\": \"Venice.\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    ```\n",
    "\n",
    "**Example 2:**\n",
    "\n",
    "*   **Input Question:** What were Joseph Black's main fields of study?\n",
    "*   **Input Answer:** Chemistry and Medicine.\n",
    "*   **Instruction:** Generate 2 new Q&A pairs about the entity \"Chemistry\", excluding \"Joseph Black\". Output in JSON.\n",
    "\n",
    "*   **Output:**\n",
    "    ```json\n",
    "    {\n",
    "      \"primary_subject\": \"Joseph Black\",\n",
    "      \"target_entity\": \"Chemistry\",\n",
    "      \"generated_qa_pairs\": [\n",
    "        {\n",
    "          \"question\": \"What is the chemical symbol for Gold?\",\n",
    "          \"answer\": \"Au.\"\n",
    "        },\n",
    "        {\n",
    "          \"question\": \"What does the pH scale measure?\",\n",
    "          \"answer\": \"The acidity or alkalinity of a solution.\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    ```\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4d02fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "**Primary Subject:** {person_name}\n",
    "**Input Question:** {input_question}\n",
    "**Input Answer:** {input_answer}\n",
    "**Instruction:** Generate 3 new Q&A pairs about the entity \"{input_answer}\", excluding \"{person_name}\". Output in JSON.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fabfd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac9912",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['entity_test_questions'] = ''\n",
    "for i, row in final_df.iterrows():\n",
    "    name = row['title']\n",
    "    question = row['question']\n",
    "    answer = row['answer']\n",
    "    sys_prompt = sys_prompt\n",
    "    prompt_text = prompt.format(person_name=name, input_question=question, input_answer=answer)\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": prompt_text},]\n",
    "    print(\"Now generating for \", name)\n",
    "    response = client.chat.completions.create(\n",
    "        messages= messages,\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.1,\n",
    "        max_completion_tokens=1000\n",
    "    )\n",
    "    response_text = response.choices[0].message.content\n",
    "    final_df.at[i, 'entity_questions'] = response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b721e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re # Import the regular expression module\n",
    "\n",
    "\n",
    "json_pattern = re.compile(r\"```json\\s*(.*?)\\s*```\", re.DOTALL)\n",
    "\n",
    "# List to hold the processed data rows\n",
    "processed_rows = []\n",
    "\n",
    "# Iterate through each row of the original DataFrame\n",
    "for index, row in final_df.iterrows():\n",
    "    raw_text = row['entity_test_questions']\n",
    "\n",
    "    # Ensure it's a string before proceeding\n",
    "    if not isinstance(raw_text, str):\n",
    "        print(f\"Skipping row {index}: Input is not a string ({type(raw_text)}).\")\n",
    "        continue # Skip non-string entries\n",
    "\n",
    "    # Find the JSON block using regex\n",
    "    match = json_pattern.search(raw_text)\n",
    "\n",
    "    if match:\n",
    "        # Extract the captured group (the JSON string itself)\n",
    "        json_string = match.group(1).strip() # .strip() removes potential leading/trailing whitespace within the captured group\n",
    "\n",
    "        # Handle cases where the extracted string is empty\n",
    "        if not json_string:\n",
    "            print(f\"Skipping row {index}: Found JSON markers but content between them is empty.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Parse the extracted JSON string\n",
    "            data = json.loads(json_string)\n",
    "\n",
    "            # Extract top-level data (use .get() for safety)\n",
    "            primary_subject = data.get('primary_subject')\n",
    "            target_entity = data.get('target_entity')\n",
    "            qa_pairs = data.get('generated_qa_pairs', [])\n",
    "\n",
    "            # Handle Multiple Q&A Pairs\n",
    "            if qa_pairs:\n",
    "                for qa_pair in qa_pairs:\n",
    "                     if isinstance(qa_pair, dict):\n",
    "                         question = qa_pair.get('question')\n",
    "                         answer = qa_pair.get('answer')\n",
    "                         processed_rows.append({\n",
    "                             'primary_subject': primary_subject,\n",
    "                             'target_entity': target_entity,\n",
    "                             'entity_question': question,\n",
    "                             'entity_answer': answer\n",
    "                         })\n",
    "                     else:\n",
    "                         print(f\"Skipping malformed QA pair in row {index}: {qa_pair}\")\n",
    "            else:\n",
    "                print(f\"Row {index} has no 'generated_qa_pairs'.\")\n",
    "                # Optionally add a row with None Q/A if needed:\n",
    "                # processed_rows.append({\n",
    "                #      'primary_subject': primary_subject,\n",
    "                #      'target_entity': target_entity,\n",
    "                #      'entity_question': None,\n",
    "                #      'entity_answer': None\n",
    "                # })\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipping row {index}: Invalid JSON format inside the ```json ``` block.\")\n",
    "            # You could print the problematic json_string here for debugging:\n",
    "            # print(f\"Problematic JSON string:\\n{json_string}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping row {index}: An unexpected error occurred during processing - {e}\")\n",
    "\n",
    "    else:\n",
    "        # No JSON block found in the string\n",
    "        print(f\"Skipping row {index}: Could not find '```json ... ```' block.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249330ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(processed_rows)\n",
    "# Rename columns\n",
    "new_df = new_df.rename(columns={'primary_subject': 'title', 'target_entity': 'answer'})\n",
    "\n",
    "# Merge with final_df\n",
    "merged_df = pd.merge(final_df, new_df, on=['title', 'answer'], how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6692527",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.rename(columns ={'question': 'forget_question', 'answer': 'forget_answer', 'entity_question': 'question', 'entity_answer': 'answer'})\n",
    "merged_df.drop(columns=['entity_questions', 'wikipage', 'paraphrased_question'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe271db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('direct_test_qa.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melu",
   "language": "python",
   "name": "melu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
